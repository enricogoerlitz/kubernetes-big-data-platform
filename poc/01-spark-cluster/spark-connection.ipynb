{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba187774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 10:49:34 WARN Utils: Your hostname, MacBook-Air-von-Enrico.local resolves to a loopback address: 127.0.0.1; using 192.168.0.5 instead (on interface en0)\n",
      "25/04/20 10:49:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 10:49:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbindung zum Spark-Cluster erfolgreich hergestellt!\n",
      "Cluster-Informationen:\n",
      "Spark Version: 3.5.5\n",
      "Master: spark://9.163.152.33:7077,9.163.152.33:7078\n",
      "Verfügbare Cores: 2\n",
      "\n",
      "Erstellter Test-Dataframe:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 10:49:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/04/20 10:49:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/04/20 10:50:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/04/20 10:50:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "SPARK_HOST = \"9.163.152.33\"\n",
    "\n",
    "\n",
    "def test_spark_connection():\n",
    "    # Initialisiere SparkSession mit URL zu beiden Master-Nodes\n",
    "    # Im Fehlerfall wird automatisch zum anderen Master gewechselt\n",
    "    spark: SparkSession = SparkSession.builder \\\n",
    "        .appName(\"SparkConnectionTest-3\") \\\n",
    "        .master(f\"spark://{SPARK_HOST}:7077,{SPARK_HOST}:7078\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Einfachen Dataframe erstellen und abfragen\n",
    "    data = [(\"Alice\", 1), (\"Bob\", 2), (\"Carol\", 3)]\n",
    "    columns = [\"name\", \"id\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    # Dataframe anzeigen\n",
    "    print(\"Verbindung zum Spark-Cluster erfolgreich hergestellt!\")\n",
    "    print(\"Cluster-Informationen:\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Master: {spark.sparkContext.master}\")\n",
    "    print(f\"Verfügbare Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "    print(\"\\nErstellter Test-Dataframe:\")\n",
    "    df.show()\n",
    "    \n",
    "    # Failover testen (optional)\n",
    "    print(\"\\nFühre eine einfache Berechnung durch...\")\n",
    "    df.selectExpr(\"count(*) as total\").show()\n",
    "    \n",
    "    # Spark-Session beenden\n",
    "    spark.stop()\n",
    "    print(\"Spark-Session erfolgreich beendet\")\n",
    "\n",
    "# Verbindung testen\n",
    "test_spark_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056590ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 10:42:04 WARN Utils: Your hostname, MacBook-Air-von-Enrico.local resolves to a loopback address: 127.0.0.1; using 192.168.0.5 instead (on interface en0)\n",
      "25/04/20 10:42:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 10:42:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbindung zum Spark-Cluster erfolgreich hergestellt!\n",
      "Cluster-Informationen:\n",
      "Spark Version: 3.5.5\n",
      "Master: spark://9.163.152.33:7077,9.163.152.33:7078\n",
      "Verfügbare Cores: 2\n",
      "\n",
      "Erstellter Test-Dataframe:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 10:42:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/04/20 10:42:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/04/20 10:42:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/04/20 10:42:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/04/20 10:43:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/04/20 10:43:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "SPARK_HOST = \"9.163.152.33\"\n",
    "\n",
    "\n",
    "def test_spark_connection():\n",
    "    # Initialisiere SparkSession mit URL zu beiden Master-Nodes\n",
    "    # Im Fehlerfall wird automatisch zum anderen Master gewechselt\n",
    "    spark: SparkSession = SparkSession.builder \\\n",
    "        .appName(\"SparkConnectionTest-2\") \\\n",
    "        .master(f\"spark://{SPARK_HOST}:7077,{SPARK_HOST}:7078\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Einfachen Dataframe erstellen und abfragen\n",
    "    test_data = [(\"Test\", 1), (\"Connection\", 2), (\"Success\", 3)]\n",
    "    columns = [\"word\", \"count\"]\n",
    "    df = spark.createDataFrame(test_data, columns)\n",
    "    \n",
    "    # Dataframe anzeigen\n",
    "    print(\"Verbindung zum Spark-Cluster erfolgreich hergestellt!\")\n",
    "    print(\"Cluster-Informationen:\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Master: {spark.sparkContext.master}\")\n",
    "    print(f\"Verfügbare Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "    print(\"\\nErstellter Test-Dataframe:\")\n",
    "    df.show()\n",
    "    \n",
    "    # Failover testen (optional)\n",
    "    print(\"\\nFühre eine einfache Berechnung durch...\")\n",
    "    result = spark.range(1, 1000000).selectExpr(\"sum(id)\").collect()\n",
    "    print(f\"Ergebnis: {result[0][0]}\")\n",
    "    \n",
    "    # Spark-Session beenden\n",
    "    spark.stop()\n",
    "    print(\"Spark-Session erfolgreich beendet\")\n",
    "\n",
    "# Verbindung testen\n",
    "test_spark_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
